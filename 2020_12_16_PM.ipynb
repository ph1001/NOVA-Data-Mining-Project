{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "- Deal with records that have RDATE before ADATE\n",
    "- (?) (Probably doesn't make sense) Do something with this information (from metadata text file):\n",
    "                            LL mailings had labels only\n",
    "                            WL mailings had labels only\n",
    "                            CC mailings are calendars with stickers but do\n",
    "                               not have labels\n",
    "                            FS mailings are blank cards that fold into\n",
    "                               thirds with labels\n",
    "                            NK mailings are blank cards with labels\n",
    "                            SK mailings are blank cards with labels\n",
    "                            TK mailings have thank you printed on the\n",
    "                               outside with labels\n",
    "                            GK mailings are general greeting cards (an\n",
    "                               assortment of birthday, sympathy, blank, & get\n",
    "                               well) with labels\n",
    "                            XK mailings are Christmas cards with labels\n",
    "                            X1 mailings have labels and a notepad\n",
    "                            G1 mailings have labels and a notepad\n",
    "- Use the variable that is most correlated with 'Age' ('Age' is yet to be created from 'DOB') to fill in the missing values of 'Age' (using a linear model for example)\n",
    "- As a final check for outlier detection, use DBSCAN to see if all outliers were excluded\n",
    "- Henrique's notes:\n",
    "    - I had one note in my notebook we should turn ODATE into number of months for RFA matters maybe and DOB to days\n",
    "        - -> <span style=\"color:red\">Turned all date features into days relative to ADATE_2</span>\n",
    "    - Another one saying that NOEXCH could be remove. Check it out a see if you agree\n",
    "        - <span style=\"color:red\">This one doesn't seem too bad to me so far actually</span>\n",
    "- Decide what to do with 'HOMEOWNR' (Home Owner Flag, H = Home owner, U = Unknown)\n",
    "    - pd.unique(donors.HOMEOWNR) ---> array([nan, 'H', 'U'], dtype=object)\n",
    "    - Could be incuded as: \"Home owner\" = 1, \"No home owner\" = 0, \"Unknown\" = nan\n",
    "    - But does it make sense to use binary variables for clustering?\n",
    "- What was done in the \"clustering\" lab: Use DBSCAN to remove outliers. Do the clustering on the inliers. Then in the end add each outlier to the closest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Insights from the Q&A on Thursday, 10. Dec. 2020:</span>\n",
    "\n",
    "<span style=\"color:red\">**See text file \"Notes Q&A 10. Dec. 2020\" in folder \"PDFs and notes\"**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "from datetime import date\n",
    "import math\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Seeting seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off warnings\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data and metadata/donors.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2044accf9165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdonors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data and metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'donors.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data and metadata/donors.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "donors = pd.read_csv(os.path.join('Data and metadata', 'donors.csv'), sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original features in a list\n",
    "features_orig = list(donors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for checking the types of all elements of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'ADATE_2'\n",
    "type_items_to_retieve = float\n",
    "type_set = set()\n",
    "list_ = []\n",
    "for item in donors[feature]:\n",
    "    type_ = type(item)\n",
    "    type_set.add(type_)\n",
    "    if type_ == type_items_to_retieve:\n",
    "        list_.append(item)\n",
    "print('Types present in this feature:')\n",
    "print(type_set)\n",
    "print('Items that have type', type_items_to_retieve, ':')\n",
    "print(pd.unique(list_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate metric features from non-metric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top of remaining dataset\n",
    "donors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain information of the type of a certain feature\n",
    "feature = 'HOMEOWNR'\n",
    "\n",
    "# Print its type\n",
    "print('dtype of feature', feature, ':', donors.dtypes[feature])\n",
    "\n",
    "# Take a closer look at the first non nan element of it\n",
    "first_non_na_element = donors[feature][~donors[feature].isna()].iloc[0]\n",
    "print('First non nan element of this feature:', first_non_na_element)\n",
    "print('Class of the first element of this feature:', type(first_non_na_element))\n",
    "\n",
    "# View the unique values of this feature\n",
    "# print('Sorted unique values of feature', feature, ':', np.sort(pd.unique(donors[feature])))\n",
    "print('Unique values of feature', feature, ':', pd.unique(donors[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the metric features\n",
    "# Including dates and other variables that might still be Strings at this point\n",
    "# Will use 'to_num' on all metric features in the next step\n",
    "metric_features = ['ODATEDW',\n",
    " 'DOB',\n",
    " 'NUMCHLD',\n",
    " 'INCOME',\n",
    " 'WEALTH1',\n",
    " 'HIT',\n",
    "                   # Above: Information about the donor\n",
    "                   # Below: About the number of times the donor has responded to other types of mail order offers\n",
    " 'MBCRAFT',\n",
    " 'MBGARDEN',\n",
    " 'MBBOOKS',\n",
    " 'MBCOLECT',\n",
    " 'MAGFAML',\n",
    " 'MAGFEM',\n",
    " 'MAGMALE',\n",
    " 'PUBGARDN',\n",
    " 'PUBCULIN',\n",
    " 'PUBHLTH',\n",
    " 'PUBDOITY',\n",
    " 'PUBNEWFN',\n",
    " 'PUBPHOTO',\n",
    " 'PUBOPP',\n",
    "                   # Below: Data from third party regarding the household or neighborhood (According to David Silva) \n",
    " 'MALEMILI',\n",
    " 'MALEVET',\n",
    " 'VIETVETS',\n",
    " 'WWIIVETS',\n",
    " 'LOCALGOV',\n",
    " 'STATEGOV',\n",
    " 'FEDGOV',         \n",
    " 'SOLP3',\n",
    " 'SOLIH',\n",
    " 'WEALTH2',\n",
    "                   # Below: About donor's neighbourhood\n",
    " 'POP901',\n",
    " 'POP902',\n",
    " 'POP903',\n",
    " 'POP90C1',\n",
    " 'POP90C2',\n",
    " 'POP90C3',\n",
    " 'POP90C4',\n",
    " 'POP90C5',\n",
    " 'ETH1',\n",
    " 'ETH2',\n",
    " 'ETH3',\n",
    " 'ETH4',\n",
    " 'ETH5',\n",
    " 'ETH6',\n",
    " 'ETH7',\n",
    " 'ETH8',\n",
    " 'ETH9',\n",
    " 'ETH10',\n",
    " 'ETH11',\n",
    " 'ETH12',\n",
    " 'ETH13',\n",
    " 'ETH14',\n",
    " 'ETH15',\n",
    " 'ETH16',              \n",
    " 'AGE901',\n",
    " 'AGE902',\n",
    " 'AGE903',\n",
    " 'AGE904',\n",
    " 'AGE905',\n",
    " 'AGE906',\n",
    " 'AGE907',\n",
    " 'CHIL1',\n",
    " 'CHIL2',\n",
    " 'CHIL3',\n",
    " 'AGEC1',\n",
    " 'AGEC2',\n",
    " 'AGEC3',\n",
    " 'AGEC4',\n",
    " 'AGEC5',\n",
    " 'AGEC6',\n",
    " 'AGEC7',\n",
    " 'CHILC1',\n",
    " 'CHILC2',\n",
    " 'CHILC3',\n",
    " 'CHILC4',\n",
    " 'CHILC5',\n",
    " 'HHAGE1',\n",
    " 'HHAGE2',\n",
    " 'HHAGE3',\n",
    " 'HHN1',\n",
    " 'HHN2',\n",
    " 'HHN3',\n",
    " 'HHN4',\n",
    " 'HHN5',\n",
    " 'HHN6',\n",
    " 'MARR1',\n",
    " 'MARR2',\n",
    " 'MARR3',\n",
    " 'MARR4',           \n",
    " 'HHP1',\n",
    " 'HHP2',\n",
    " 'DW1',\n",
    " 'DW2',\n",
    " 'DW3',\n",
    " 'DW4',\n",
    " 'DW5',\n",
    " 'DW6',\n",
    " 'DW7',\n",
    " 'DW8',\n",
    " 'DW9',\n",
    " 'HV1',\n",
    " 'HV2',\n",
    " 'HV3',\n",
    " 'HV4',\n",
    " 'HU1',\n",
    " 'HU2',\n",
    " 'HU3',\n",
    " 'HU4',\n",
    " 'HU5',                  \n",
    " 'HHD1',\n",
    " 'HHD2',\n",
    " 'HHD3',\n",
    " 'HHD4',\n",
    " 'HHD5',\n",
    " 'HHD6',\n",
    " 'HHD7',\n",
    " 'HHD8',\n",
    " 'HHD9',\n",
    " 'HHD10',\n",
    " 'HHD11',\n",
    " 'HHD12',\n",
    " 'ETHC1',\n",
    " 'ETHC2',\n",
    " 'ETHC3',\n",
    " 'ETHC4',\n",
    " 'ETHC5',\n",
    " 'ETHC6',\n",
    " 'HVP1',\n",
    " 'HVP2',\n",
    " 'HVP3',\n",
    " 'HVP4',\n",
    " 'HVP5',\n",
    " 'HVP6',                  \n",
    " 'HUR1',\n",
    " 'HUR2',\n",
    " 'RHP1',\n",
    " 'RHP2',\n",
    " 'RHP3',\n",
    " 'RHP4',\n",
    " 'HUPA1',\n",
    " 'HUPA2',\n",
    " 'HUPA3',\n",
    " 'HUPA4',\n",
    " 'HUPA5',\n",
    " 'HUPA6',\n",
    " 'HUPA7',\n",
    " 'RP1',\n",
    " 'RP2',\n",
    " 'RP3',\n",
    " 'RP4',                 \n",
    " 'IC1',\n",
    " 'IC2',\n",
    " 'IC3',\n",
    " 'IC4',\n",
    " 'IC5',\n",
    " 'IC6',\n",
    " 'IC7',\n",
    " 'IC8',\n",
    " 'IC9',\n",
    " 'IC10',\n",
    " 'IC11',\n",
    " 'IC12',\n",
    " 'IC13',\n",
    " 'IC14',\n",
    " 'IC15',\n",
    " 'IC16',\n",
    " 'IC17',\n",
    " 'IC18',\n",
    " 'IC19',\n",
    " 'IC20',\n",
    " 'IC21',\n",
    " 'IC22',\n",
    " 'IC23',           \n",
    " 'HHAS1',\n",
    " 'HHAS2',\n",
    " 'HHAS3',\n",
    " 'HHAS4',\n",
    " 'MC1',\n",
    " 'MC2',\n",
    " 'MC3',\n",
    " 'TPE1',\n",
    " 'TPE2',\n",
    " 'TPE3',\n",
    " 'TPE4',\n",
    " 'TPE5',\n",
    " 'TPE6',\n",
    " 'TPE7',\n",
    " 'TPE8',\n",
    " 'TPE9',\n",
    " 'PEC1',\n",
    " 'PEC2',\n",
    " 'TPE10',\n",
    " 'TPE11',\n",
    " 'TPE12',\n",
    " 'TPE13',\n",
    " 'LFC1',\n",
    " 'LFC2',\n",
    " 'LFC3',\n",
    " 'LFC4',\n",
    " 'LFC5',\n",
    " 'LFC6',\n",
    " 'LFC7',\n",
    " 'LFC8',\n",
    " 'LFC9',\n",
    " 'LFC10',\n",
    " 'OCC1',\n",
    " 'OCC2',\n",
    " 'OCC3',\n",
    " 'OCC4',\n",
    " 'OCC5',\n",
    " 'OCC6',\n",
    " 'OCC7',\n",
    " 'OCC8',\n",
    " 'OCC9',\n",
    " 'OCC10',\n",
    " 'OCC11',\n",
    " 'OCC12',\n",
    " 'OCC13',\n",
    " 'EIC1',\n",
    " 'EIC2',\n",
    " 'EIC3',\n",
    " 'EIC4',\n",
    " 'EIC5',\n",
    " 'EIC6',\n",
    " 'EIC7',\n",
    " 'EIC8',\n",
    " 'EIC9',\n",
    " 'EIC10',\n",
    " 'EIC11',\n",
    " 'EIC12',\n",
    " 'EIC13',\n",
    " 'EIC14',\n",
    " 'EIC15',\n",
    " 'EIC16',\n",
    " 'OEDC1',\n",
    " 'OEDC2',\n",
    " 'OEDC3',\n",
    " 'OEDC4',\n",
    " 'OEDC5',\n",
    " 'OEDC6',\n",
    " 'OEDC7',\n",
    " 'EC1',\n",
    " 'EC2',\n",
    " 'EC3',\n",
    " 'EC4',\n",
    " 'EC5',\n",
    " 'EC6',\n",
    " 'EC7',\n",
    " 'EC8',\n",
    " 'SEC1',\n",
    " 'SEC2',\n",
    " 'SEC3',\n",
    " 'SEC4',\n",
    " 'SEC5',\n",
    " 'AFC1',\n",
    " 'AFC2',\n",
    " 'AFC3',\n",
    " 'AFC4',\n",
    " 'AFC5',\n",
    " 'AFC6',\n",
    " 'VC1',\n",
    " 'VC2',\n",
    " 'VC3',\n",
    " 'VC4',\n",
    " 'ANC1',\n",
    " 'ANC2',\n",
    " 'ANC3',\n",
    " 'ANC4',\n",
    " 'ANC5',\n",
    " 'ANC6',\n",
    " 'ANC7',\n",
    " 'ANC8',\n",
    " 'ANC9',\n",
    " 'ANC10',\n",
    " 'ANC11',\n",
    " 'ANC12',\n",
    " 'ANC13',\n",
    " 'ANC14',\n",
    " 'ANC15',\n",
    " 'POBC1',\n",
    " 'POBC2',\n",
    " 'LSC1',\n",
    " 'LSC2',\n",
    " 'LSC3',\n",
    " 'LSC4',\n",
    " 'VOC1',\n",
    " 'VOC2',\n",
    " 'VOC3',\n",
    " 'HC1',\n",
    " 'HC2',\n",
    " 'HC3',\n",
    " 'HC4',\n",
    " 'HC5',\n",
    " 'HC6',\n",
    " 'HC7',\n",
    " 'HC8',\n",
    " 'HC9',\n",
    " 'HC10',\n",
    " 'HC11',\n",
    " 'HC12',\n",
    " 'HC13',\n",
    " 'HC14',\n",
    " 'HC15',\n",
    " 'HC16',\n",
    " 'HC17',\n",
    " 'HC18',\n",
    " 'HC19',\n",
    " 'HC20',\n",
    " 'HC21',\n",
    " 'MHUC1',\n",
    " 'MHUC2',\n",
    " 'AC1',\n",
    " 'AC2',\n",
    "                   # Above: About donor's neighbourhood      \n",
    "                   # Below: Date promotion X was mailed\n",
    " 'ADATE_2',\n",
    " 'ADATE_3',\n",
    " 'ADATE_4',\n",
    " 'ADATE_5',\n",
    " 'ADATE_6',\n",
    " 'ADATE_7',\n",
    " 'ADATE_8',\n",
    " 'ADATE_9',\n",
    " 'ADATE_10',\n",
    " 'ADATE_11',\n",
    " 'ADATE_12',\n",
    " 'ADATE_13',\n",
    " 'ADATE_14',\n",
    " 'ADATE_15',\n",
    " 'ADATE_16',\n",
    " 'ADATE_17',\n",
    " 'ADATE_18',\n",
    " 'ADATE_19',\n",
    " 'ADATE_20',\n",
    " 'ADATE_21',\n",
    " 'ADATE_22',\n",
    " 'ADATE_23',\n",
    " 'ADATE_24',\n",
    "                   # Below: Information about how many promotions donor has received\n",
    " 'CARDPROM',\n",
    " 'MAXADATE',\n",
    " 'NUMPROM',\n",
    " 'CARDPM12',\n",
    " 'NUMPRM12',\n",
    "                   # Below: Date the donation was received\n",
    " 'RDATE_3',\n",
    " 'RDATE_4',\n",
    " 'RDATE_5',\n",
    " 'RDATE_6',\n",
    " 'RDATE_7',\n",
    " 'RDATE_8',\n",
    " 'RDATE_9',\n",
    " 'RDATE_10',\n",
    " 'RDATE_11',\n",
    " 'RDATE_12',\n",
    " 'RDATE_13',\n",
    " 'RDATE_14',\n",
    " 'RDATE_15',\n",
    " 'RDATE_16',\n",
    " 'RDATE_17',\n",
    " 'RDATE_18',\n",
    " 'RDATE_19',\n",
    " 'RDATE_20',\n",
    " 'RDATE_21',\n",
    " 'RDATE_22',\n",
    " 'RDATE_23',\n",
    " 'RDATE_24',\n",
    "                   # Below: Dollar amount of the donation\n",
    " 'RAMNT_3',\n",
    " 'RAMNT_4',\n",
    " 'RAMNT_5',\n",
    " 'RAMNT_6',\n",
    " 'RAMNT_7',\n",
    " 'RAMNT_8',\n",
    " 'RAMNT_9',\n",
    " 'RAMNT_10',\n",
    " 'RAMNT_11',\n",
    " 'RAMNT_12',\n",
    " 'RAMNT_13',\n",
    " 'RAMNT_14',\n",
    " 'RAMNT_15',\n",
    " 'RAMNT_16',\n",
    " 'RAMNT_17',\n",
    " 'RAMNT_18',\n",
    " 'RAMNT_19',\n",
    " 'RAMNT_20',\n",
    " 'RAMNT_21',\n",
    " 'RAMNT_22',\n",
    " 'RAMNT_23',\n",
    " 'RAMNT_24',\n",
    "                   # Below: Summary variables for this donor\n",
    " 'RAMNTALL',\n",
    " 'NGIFTALL',\n",
    " 'CARDGIFT',\n",
    " 'MINRAMNT',\n",
    " 'MINRDATE',\n",
    " 'MAXRAMNT',\n",
    " 'MAXRDATE',\n",
    " 'LASTGIFT',\n",
    " 'LASTDATE',\n",
    " 'FISTDATE',\n",
    " 'NEXTDATE',\n",
    " 'TIMELAG',\n",
    " 'AVGGIFT']\n",
    "\n",
    "print('Number of metric features:', len(metric_features))\n",
    "\n",
    "# Save this oroginal metric features list\n",
    "metric_features_orig = metric_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the non metrics features by excluding the metric ones\n",
    "non_metric_features = donors.columns.drop(metric_features).to_list()\n",
    "\n",
    "print('Number of non-metric features:', len(non_metric_features))\n",
    "\n",
    "# Save this oroginal metric features list\n",
    "non_metric_features_orig = non_metric_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat features where \" \" (space) carries a meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAILCODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'MAILCODE'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Address is OK\", inplace=True)\n",
    "donors[feature].replace(\"B\", \"Bad Address\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOEXCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'NOEXCH'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"can be exchanged\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"do not exchange\", inplace=True)\n",
    "# Assumption: 1 = do not exchange, 0 = can be exchanged (Makes sense because of the variable name)\n",
    "donors[feature].replace('0', \"can be exchanged\", inplace=True)\n",
    "donors[feature].replace('1', \"do not exchange\", inplace=True)\n",
    "donors[feature].replace(0, \"can be exchanged\", inplace=True)\n",
    "donors[feature].replace(1, \"do not exchange\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECINHSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'RECINHSE'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Not an In House Record\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"Donor has given to PVA's In House program\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECP3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'RECP3'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Not a P3 Record\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"Donor has given to PVA's P3 program\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECPGVG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'RECPGVG'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Not a Planned Giving Record\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"Planned Giving Record\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECSWEEP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'RECSWEEP'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Not a Sweepstakes Record\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"Sweepstakes Record\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAJOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'MAJOR'\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature].replace(\" \", \"Not a Major Donor\", inplace=True)\n",
    "donors[feature].replace(\"X\", \"Major Donor\", inplace=True)\n",
    "pd.unique(donors[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses existance of nan values and duplictes and deal with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many duplicated observations exist\n",
    "donors.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the variables that have missing values and their missing value counts\n",
    "missing_value_counts = donors.isna().sum()[donors.isna().sum()!=0]\n",
    "missing_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values\n",
    "donors.isna().sum()[donors.isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \" \" by nans\n",
    "donors.replace(\" \", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values once more\n",
    "donors.isna().sum()[donors.isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**About 3 million \" \" (spaces) were converted to nan.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"\" by nans\n",
    "donors.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values once more\n",
    "donors.isna().sum()[donors.isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**No additional nans added in this step.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the percentage of missing values per feature and drop the features that have more than 40% mssing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list\n",
    "nan_percentage_list = []\n",
    "\n",
    "# Loop over the list of features and compute their percentage of nan values\n",
    "for feature in list(donors):\n",
    "    nan_percentage = len(donors[feature][donors[feature].isna()]) / len(donors[feature]) * 100\n",
    "    nan_percentage_list.append(nan_percentage)\n",
    "    \n",
    "# Create a look-up table\n",
    "nan_percentage_series = pd.Series(data=nan_percentage_list, index=list(donors))\n",
    "nan_percentage_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the features that have a higher amount of missing values\n",
    "lower_nan_percentage = nan_percentage_series[nan_percentage_series<=40].index.tolist()\n",
    "\n",
    "# Get a list of the features that have a lower amount of missing values\n",
    "higher_nan_percentage = nan_percentage_series[nan_percentage_series>40].index.tolist()\n",
    "\n",
    "# For report: Save the names of the features with the lower nan percentage in the variable \n",
    "# \"features_dropped_due_to_nans\"\n",
    "features_dropped_due_to_nans = higher_nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_1 = donors[lower_nan_percentage]\n",
    "donors_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept the changes\n",
    "donors = donors_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_features(original_order, shuffled):\n",
    "    ordered = []\n",
    "    for feature in original_order:\n",
    "        if feature in shuffled:\n",
    "            ordered.append(feature)\n",
    "    return ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'metric_features' list\n",
    "\n",
    "# Print the number of metric features before the removal\n",
    "print('Number of metric features before removal:', len(metric_features))\n",
    "\n",
    "# Get the features that are metric features and still exist in our dataset\n",
    "metric_features = list(set(metric_features).intersection(set(donors)))\n",
    "\n",
    "# And sort them according to our original order\n",
    "metric_features = sort_features(metric_features_orig, metric_features)\n",
    "\n",
    "# Print the number of metric features after the removal\n",
    "print('Number of metric features afer removal:', len(metric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'non_metric_features' list\n",
    "\n",
    "# Print the number of metric features before the removal\n",
    "print('Number of non-metric features before removal:', len(non_metric_features))\n",
    "\n",
    "# Get the features that are metric features and still exist in our dataset\n",
    "non_metric_features = list(set(non_metric_features).intersection(set(donors)))\n",
    "\n",
    "# And sort them according to our original order\n",
    "non_metric_features = sort_features(non_metric_features_orig, non_metric_features)\n",
    "\n",
    "# Print the number of metric features after the removal\n",
    "print('Number of non-metric features afer removal:', len(non_metric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values once more\n",
    "donors.isna().sum()[donors.isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Before: About 8 million nans. Now: Less than 1 million nans.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform columns containing dates to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_string_to_date_if_not_nan(x):\n",
    "    \"\"\"This function checks if something is a string, and if so, converts it into a datetime object\"\"\"\n",
    "    # If it's not a nan, but a string\n",
    "    if type(x)==str:\n",
    "        x = datetime.datetime.strptime(x, '%Y-%m-%d').date()\n",
    "    # if it's a nan, change to NaT\n",
    "    # else:\n",
    "    #     x = pd.NaT\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for converting series containing strings to series containing datetime objects\n",
    "def series_string_to_date(series):\n",
    "    \"\"\"This function turns a pandas series that consists of String values into a pandas series containing \n",
    "    datetime objects\"\"\"\n",
    "    series_datetime = series.map(lambda x: series_string_to_date_if_not_nan(x))\n",
    "    return series_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_string_col_by_date_col(df, colname):\n",
    "    \"\"\"This function uses 'series_string_to_date' for replacing a string column by a datetime object column\"\"\"\n",
    "    df[colname] = series_string_to_date(df[colname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for checking data types\n",
    "if False:\n",
    "    labels_types = pd.DataFrame({'Labels':list(donors), 'Data types':list(donors.dtypes)})\n",
    "    labels_types[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels who's columns should be changed to datetime\n",
    "date_features = [\n",
    "    'ODATEDW',\n",
    "    'DOB',\n",
    "    'ADATE_2',\n",
    "    'ADATE_3',\n",
    "    'ADATE_4',\n",
    "    'ADATE_5',\n",
    "    'ADATE_6',\n",
    "    'ADATE_7',\n",
    "    'ADATE_8',\n",
    "    'ADATE_9',\n",
    "    'ADATE_10',\n",
    "    'ADATE_11',\n",
    "    'ADATE_12',\n",
    "    'ADATE_13',\n",
    "    'ADATE_14',\n",
    "    'ADATE_15',\n",
    "    'ADATE_16',\n",
    "    'ADATE_17',\n",
    "    'ADATE_18',\n",
    "    'ADATE_19',\n",
    "    'ADATE_20',\n",
    "    'ADATE_21',\n",
    "    'ADATE_22',\n",
    "    'ADATE_23',\n",
    "    'ADATE_24',\n",
    "    'MAXADATE',\n",
    "    'RDATE_3',\n",
    "    'RDATE_4',\n",
    "    'RDATE_5',\n",
    "    'RDATE_6',\n",
    "    'RDATE_7',\n",
    "    'RDATE_8',\n",
    "    'RDATE_9',\n",
    "    'RDATE_10',\n",
    "    'RDATE_11',\n",
    "    'RDATE_12',\n",
    "    'RDATE_13',\n",
    "    'RDATE_14',\n",
    "    'RDATE_15',\n",
    "    'RDATE_16',\n",
    "    'RDATE_17',\n",
    "    'RDATE_18',\n",
    "    'RDATE_19',\n",
    "    'RDATE_20',\n",
    "    'RDATE_21',\n",
    "    'RDATE_22',\n",
    "    'RDATE_23',\n",
    "    'RDATE_24',\n",
    "    'MINRDATE',\n",
    "    'MAXRDATE',\n",
    "    'LASTDATE',\n",
    "    'FISTDATE',\n",
    "    'NEXTDATE',\n",
    "]\n",
    "\n",
    "# Save this original list of date features\n",
    "date_features_orig = date_features.copy()\n",
    "\n",
    "# Get the features that are date features and still exist in our dataset\n",
    "date_features = list(set(date_features).intersection(set(donors)))\n",
    "\n",
    "# And sort them according to our original order\n",
    "date_features = sort_features(date_features_orig, date_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for checking the existing datatypes in a given columns\n",
    "# This needs some work. Doesn't always work properly\n",
    "if False:\n",
    "    feature = 'NUMCHLD'\n",
    "    unique_types = set()\n",
    "    str_indices = []\n",
    "    float_indices = []\n",
    "    other_indices = []\n",
    "    for i in range(len(donors[feature])):\n",
    "        if type(donors[feature][i])==str:\n",
    "                str_indices.append(i)\n",
    "        if type(donors[feature][i])==float:\n",
    "                float_indices.append(i)\n",
    "        else:\n",
    "                other_indices.append(i)\n",
    "        unique_types.add(type(donors[feature][i]))\n",
    "    nans = donors[feature][float_indices]\n",
    "    strings = donors[feature][str_indices]\n",
    "    print('Strings:', strings)\n",
    "    print('Floats (can be NaNs):', nans)\n",
    "    print('Contains the following data types:', unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions defined above to change all non-nan values of the columns in 'date_features' datetime objects\n",
    "for label_to_change in date_features:\n",
    "    replace_string_col_by_date_col(donors, label_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it worked properly\n",
    "for i in range(len(date_features)):\n",
    "    print(type(donors[date_features[i]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if time calculations work properly with these columns\n",
    "now = date.today()\n",
    "now - donors['DOB']\n",
    "# Seems to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas-profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_string = str(datetime.datetime.now())\n",
    "\n",
    "profile = ProfileReport(\n",
    "    donors, \n",
    "    title='Donors Data',\n",
    "    correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": True},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "    },\n",
    "    minimal = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join('pandas-profiling', now_string)\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    profile.to_file(os.path.join(folder_path, \"pandas_profiling.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"lab04_data_visualization\" (Data Mining)\n",
    "def single_hist(feature, savefig, *n_of_bins):\n",
    "    \"\"\"This function takes feature name and produces a visualisation of the histogram of the respective feature\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    # Single Metric Variable Histogram\n",
    "    if len(n_of_bins)==0:\n",
    "        print('Number of bins: Automatic')\n",
    "        plt.hist(donors[feature])\n",
    "    else:\n",
    "        print('Number of bins:', n_of_bins[0])\n",
    "        plt.hist(donors[feature], bins = n_of_bins[0])\n",
    "    plt.title(feature, y=-0.1)\n",
    "    \n",
    "    if savefig:\n",
    "        # Save figure and include time stamp in filename\n",
    "        now_string = str(datetime.datetime.now())[0:19].replace(':', '-').replace(' ', '_')\n",
    "        folder_path = os.path.join('figures')\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = now_string + '_' + feature + '_single_histogram.png'\n",
    "        plt.savefig(os.path.join(folder_path, filename), dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"lab04_data_visualization\" (Data Mining)\n",
    "\n",
    "def many_hists(features, savefig, *n_of_bins):\n",
    "    \"\"\"This function takes a list of features and optionally a number of bins and creates histograms of these\n",
    "    features.\"\"\"\n",
    "    \n",
    "    # All Numeric Variables' Histograms in one figure\n",
    "    sns.set()\n",
    "\n",
    "    # Prepare figure. Create individual axes where each histogram will be placed\n",
    "    fig, axes = plt.subplots(2, ceil(len(features) / 2), figsize=(20, 11))\n",
    "\n",
    "    # Plot data\n",
    "    # Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "    for ax, feat in zip(axes.flatten(), features): # Notice the zip() function and flatten() method\n",
    "        \n",
    "        if len(n_of_bins)==0:\n",
    "            \n",
    "            if feat == features[0]:\n",
    "                print('Number of bins: Automatic')\n",
    "            \n",
    "            ax.hist(donors[feat])\n",
    "            \n",
    "        else:\n",
    "            if feat == features[0]:\n",
    "                print('Number of bins:', n_of_bins[0])\n",
    "            ax.hist(donors[feat], bins = n_of_bins[0])\n",
    "            \n",
    "        ax.set_title(feat, y=-0.13)\n",
    "\n",
    "    # Layout\n",
    "    # Add a centered title to the figure:\n",
    "    title = \"Numeric Variables' Histograms\"\n",
    "\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if savefig:\n",
    "        # Save figure and include time stamp, the feature names and the method in the filename\n",
    "        now_string = str(datetime.datetime.now())[0:19].replace(':', '-').replace(' ', '_')\n",
    "        folder_path = os.path.join('figures')\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = now_string + '_' +  '-'.join(features) + '_histograms_' + str(n_of_bins[0]) + '_bins.png'\n",
    "        plt.savefig(os.path.join(folder_path, filename), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_value(feature, value):\n",
    "    \"\"\"This function takes a feature name and a value and turns all elements of the respective column with this value\n",
    "    into nans.\"\"\"\n",
    "    series = donors[feature].copy()\n",
    "    series[series==value]=np.nan\n",
    "    donors[feature] = series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"lab04_data_visualization\" (Data Mining)\n",
    "\n",
    "def corr_matrix_visualisation(features, savefig, method):\n",
    "    \"\"\"This function takes a list of features and created a visualisation of their correlation matrix.\"\"\"\n",
    "    \n",
    "    # Prepare figure\n",
    "    fig = plt.figure(figsize=(25, 15))\n",
    "\n",
    "    # Obtain correlation matrix. Round the values to 4 decimal cases. Use the DataFrame corr() and round() method.\n",
    "    corr = np.round(donors[features].corr(method=method), decimals=4)\n",
    "\n",
    "    # Build annotation matrix (values above |bound| will appear annotated in the plot)\n",
    "    bound = 0\n",
    "    mask_annot = np.absolute(corr.values) >= bound\n",
    "    annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "    # Plot heatmap of the correlation matrix\n",
    "    sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "                fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "    # Layout\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    fig.suptitle('Correlation Matrix' + ' (' + method + ')', fontsize=20)\n",
    "\n",
    "    if savefig:\n",
    "        # Save figure and include time stamp, the feature names and the method in the filename\n",
    "        now_string = str(datetime.datetime.now())[0:19].replace(':', '-').replace(' ', '_')\n",
    "        folder_path = os.path.join('figures')\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = now_string + '_' +  '-'.join(features) + '_' + method + '_corr_matrix.png'\n",
    "        plt.savefig(os.path.join(folder_path, filename), dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques_nans_variance(feature):\n",
    "    \"\"\"This function takes the name of a feature and prints its unique values, its percentage of missing values and\n",
    "    its standard deviation. Then (optionally) it returns the respective column so it is displayed as output.\"\"\"\n",
    "    \n",
    "    print('Unique values:')\n",
    "    unique = pd.unique(donors[feature])\n",
    "    unique.sort()\n",
    "    print(unique)\n",
    "    print()\n",
    "    \n",
    "    print('Percentage of missing values:')\n",
    "    print(round(len(donors[feature][donors[feature].isna()])/len(donors[feature]), 4), '%')\n",
    "    print()\n",
    "    \n",
    "    print('Minimum value:', donors[feature].min(), '- Share:', \\\n",
    "          round(((len(donors[feature][donors[feature]==donors[feature].min()])) / len(donors[feature])), 6) * 100, '%')\n",
    "    \n",
    "    print('Maximum value:', donors[feature].max(), '- Share:', \\\n",
    "          round(((len(donors[feature][donors[feature]==donors[feature].max()])) / len(donors[feature])), 6) * 100, '%')\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        print('Sample standard deviation:', round(donors[feature].std(), 4))\n",
    "    except:\n",
    "        print(\"Couldn't compute a variance\")\n",
    "        \n",
    "    # return donors[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_nans_variance_min_max(features):\n",
    "    \"\"\"This function takes a list of features and computes their percentages of missing values, their sample\n",
    "    standard deviations, their Minima and Maxima and the percentage of how much the minima and maxima are in\n",
    "    this feature. Then it saves all this in a DataFrame which it returns.\"\"\"\n",
    "    nans = []\n",
    "    st_devs = []\n",
    "    mins = []\n",
    "    min_shares = []\n",
    "    maxs = []\n",
    "    max_shares = []\n",
    "    for feature in features:\n",
    "        nans.append(round(len(donors[feature][donors[feature].isna()])/len(donors[feature]), 4))\n",
    "        st_devs.append(round(donors[feature].std(), 4))\n",
    "        mins.append(donors[feature].min())\n",
    "        min_shares.append(round(((len(donors[feature][donors[feature]==donors[feature].min()])) \\\n",
    "                                 / len(donors[feature])),6) * 100)\n",
    "        maxs.append(donors[feature].max())\n",
    "        max_shares.append(round(((len(donors[feature][donors[feature]==donors[feature].max()])) \\\n",
    "                                 / len(donors[feature])),6) * 100)\n",
    "    \n",
    "    df = pd.DataFrame(data = {'Missing values [%]': nans, 'Sample standard deviation [Unit of feature]': st_devs, 'Minimum [Unit of f.]': mins,\\\n",
    "                              'Share of Minimum [%]:': min_shares, 'Maximum [Unit of f.]': maxs, 'Share of Maximum [%]': max_shares},\\\n",
    "                      index = features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_boxplots(features, savefig):\n",
    "    \n",
    "    # All Numeric Variables' Box Plots in one figure\n",
    "    sns.set()\n",
    "\n",
    "    # Prepare figure. Create individual axes where each box plot will be placed\n",
    "    fig, axes = plt.subplots(2, ceil(len(features) / 2), figsize=(20, 11))\n",
    "\n",
    "    # Plot data\n",
    "    # Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "    for ax, feat in zip(axes.flatten(), features): # Notice the zip() function and flatten() method\n",
    "        sns.boxplot(x=donors[feat], ax=ax)\n",
    "\n",
    "    # Layout\n",
    "    # Add a centered title to the figure:\n",
    "    title = \"Numeric Variables' Box Plots\"\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    if savefig:\n",
    "        # Save figure and include time stamp, the feature names and the method in the filename\n",
    "        now_string = str(datetime.datetime.now())[0:19].replace(':', '-').replace(' ', '_')\n",
    "        folder_path = os.path.join('figures')\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = now_string + '_' +  '-'.join(features) + '_boxplots.png'\n",
    "        plt.savefig(os.path.join(folder_path, filename), dpi=200)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"lab04_data_visualization\" (Data Mining)\n",
    "\n",
    "def pairwise(features, flag_kdeplot):\n",
    "    \"\"\"This function takes a list of features and creates their pairwise scatterplots and each feature's histogram.\n",
    "    If 'flag_kdeplot' = True, a Kernel Dnsity Etimation plot will be added.\"\"\"\n",
    "    \n",
    "    if flag_kdeplot:\n",
    "        print('Plotting with Kernel Density Estimation. This will increase computation time immensely.')\n",
    "    else:\n",
    "        print('Plotting without Kernel Density Estimation.')\n",
    "    \n",
    "    # Pairwise Relationship of Numerical Variables\n",
    "    sns.set()\n",
    "\n",
    "    # Setting pairplot\n",
    "    g = sns.pairplot(donors[features], diag_kind=\"hist\", corner=True)\n",
    "    \n",
    "    if flag_kdeplot:\n",
    "        g.map_lower(sns.kdeplot, levels=4, color=\".2\")\n",
    "\n",
    "    # Layout\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.suptitle(\"Pairwise Relationship of Numerical Variables\", fontsize=20)\n",
    "\n",
    "    # Save figure and include time stamp, the feature names and the method in the filename\n",
    "    now_string = str(datetime.datetime.now())[0:19].replace(':', '-').replace(' ', '_')\n",
    "    folder_path = os.path.join('figures')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    filename = now_string + '_' +  '-'.join(features) + '_pairwise_plots.png'\n",
    "    plt.savefig(os.path.join(folder_path, filename), dpi=200)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'AGE901', 'AGE902', 'AGE903', 'AGE904', 'AGE905', 'AGE906','AGE907'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AGE901', 'AGE902', 'AGE903', 'AGE904', 'AGE905', 'AGE906','AGE907']\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**None of these features have missing values. All of them have suspiciously many zeros. Assume: Missing values were saved as zeros. Remove zeros.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['AGE901', 'AGE902', 'AGE903', 'AGE904', 'AGE905', 'AGE906','AGE907']:\n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'HHAGE1', 'AGEC6', 'HHAGE2', 'HHAGE3'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['HHAGE1', 'AGEC6', 'HHAGE2', 'HHAGE3']\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**None of these features have missing values. 'HHAGE1', 'AGEC6', 'HHAGE3' have many zeros in comparison to surrounding area in their histograms. Assume: Missing values were saved as zeros. Remove zeros from 'HHAGE1', 'AGEC6', 'HHAGE3'.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['HHAGE1', 'AGEC6', 'HHAGE3']:\n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'HHN1', 'HHN2', 'HHN3', 'HHN4', 'HHN5', 'HHN6' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['HHN1', 'HHN2', 'HHN3', 'HHN4', 'HHN5', 'HHN6']\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Justifiable to remove zeros from HHN1, HHN2, HHN3 and HHN4. Same reasoning as above.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['HHN1', 'HHN2', 'HHN3', 'HHN4']:\n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'HHP1', 'HHP2', 'HHN3', 'HHN4', 'HHD1', 'RHP3' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'HHP1', # (\"Median Person Per Household\")\n",
    "    'HHP2', # (\"Average Person Per Household\")\n",
    "    'HHN3', # (\"Percent 3 or More Person Households\")\n",
    "    'HHN4', # (\"Percent 4 or More Person Households\")\n",
    "    'HHD1', # (\"Percent Households w/ Related Children\")\n",
    "    'RHP3' # (\"Median Number of Persons per Housing Unit\")\n",
    "]\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Justifiable to remove zeros from HHP1, HHP2, HHD1 and RHP3. Same reasoning as above.**<span>\n",
    "    \n",
    "<span style=\"color:red\">**Remark: RHP3 looks odd. Let's consider remving it. (Will be removed in the next step.)**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['HHP1', 'HHP2', 'HHD1', 'RHP3']:\n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns  'RP1', 'RP2', 'RP3', 'RP4', 'HV4' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'RP1', # (\"Percent Renters Paying >= $500 per Month\")\n",
    "    'RP2', # (\"Percent Renters Paying >= $400 per Month\")\n",
    "    'RP3', # (\"Percent Renters Paying >= $300 per Month\")\n",
    "    'RP4', # (\"Percent Renters Paying >= $200 per Month\")\n",
    "    'HV4', # (\"Average Contract Rent in hundreds\")\n",
    "]\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Justifiable to remove zeros from RP3, RP4. Same reasoning as above.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['RP3', 'RP4']:\n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'IC1', 'IC2', 'IC3', 'IC4', 'IC5' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'IC1', # (\"Median Household Income in hundreds\")\n",
    "    'IC2', # (\"Median Family Income in hundreds\")\n",
    "    'IC3', # (\"Average Household Income in hundreds\")\n",
    "    'IC4', # (\"Average Family Income in hundreds\")\n",
    "    'IC5', #(\"Per Capita Income\")\n",
    "]\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Justifiable to remove zeros from all of them. Same reasoning as above.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['IC1','IC2','IC3','IC4','IC5']: \n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'IC6', ... , 'IC23' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "households = [\n",
    "    'IC6', # (\"Percent Households w/ Income < $15,000\")\n",
    "    'IC7', # (\"Percent Households w/ Income $15,000 - $24,999\")\n",
    "    'IC8', # (\"Percent Households w/ Income $25,000 - $34,999\")\n",
    "    'IC9', # (\"Percent Households w/ Income $35,000 - $49,999\")\n",
    "    'IC10', # (\"Percent Households w/ Income $50,000 - $74,999\")\n",
    "    'IC11', # (\"Percent Households w/ Income $75,000 - $99,999\")\n",
    "    'IC12', # (\"Percent Households w/ Income $100,000 - $124,999\")\n",
    "    'IC13', # (\"Percent Households w/ Income $125,000 - $149,999\")\n",
    "    'IC14', # (\"Percent Households w/ Income >= $150,000\")\n",
    "]\n",
    "\n",
    "families = [\n",
    "    'IC15', # (\"Percent Families w/ Income < $15,000\")\n",
    "    'IC16', # (\"Percent Families w/ Income $15,000 - $24,999\")\n",
    "    'IC17', # (\"Percent Families w/ Income $25,000 - 34,999\")\n",
    "    'IC18', # (\"Percent Families w/ Income $35,000 - $49,999\")\n",
    "    'IC19', # (\"Percent Families w/ Income $50,000 - $74,999\")\n",
    "    'IC20', # (\"Percent Families w/ Income $75,000 - $99,999\")\n",
    "    'IC21', # (\"Percent Families w/ Income $100,000 - $124,999\")\n",
    "    'IC22', # (\"Percent Families w/ Income $125,000 - $149,999\")\n",
    "    'IC23' # (\"Percent Families w/ Income >= $150,000\")\n",
    "]\n",
    "\n",
    "features = households + families\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_hists(households, savefig, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(families, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**For the same reasoning as above, remove zeros from IC6, ..., IC10 and IC15, ... IC19. For the other ones there are probably a lot of \"false zeros\" as well, but the risk of deleting too many legitimate zeros is too high in my opinion.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in [\n",
    "    'IC6', # (\"Percent Households w/ Income < $15,000\")\n",
    "    'IC7', # (\"Percent Households w/ Income $15,000 - $24,999\")\n",
    "    'IC8', # (\"Percent Households w/ Income $25,000 - $34,999\")\n",
    "    'IC9', # (\"Percent Households w/ Income $35,000 - $49,999\")\n",
    "    'IC10', # (\"Percent Households w/ Income $50,000 - $74,999\")\n",
    "    \n",
    "    'IC15', # (\"Percent Families w/ Income < $15,000\")\n",
    "    'IC16', # (\"Percent Families w/ Income $15,000 - $24,999\")\n",
    "    'IC17', # (\"Percent Families w/ Income $25,000 - 34,999\")\n",
    "    'IC18', # (\"Percent Families w/ Income $35,000 - $49,999\")\n",
    "    'IC19', # (\"Percent Families w/ Income $50,000 - $74,999\")\n",
    "]: \n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(households, savefig, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(families, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 from columns 'HHAS1', 'HHAS2', 'HHAS3', 'HHAS4' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'HHAS1', # (\"Percent Households on Social Security\")\n",
    "    'HHAS2', # (\"Percent Households on Public Assistance\")\n",
    "    'HHAS3', # (\"Percent Households w/ Interest, Rental or Dividend Income\")\n",
    "    'HHAS4' # (\"Percent Persons Below Poverty Level\")\n",
    "]\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**For the same reasons as above, delete zeros from HHAS1, HHAS2 and HHAS3, but not HHAS4.**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in ['HHAS1','HHAS2','HHAS3',]: \n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 and 99 from columns 'LFC1', 'LFC2', 'LFC3', 'LFC4', 'LFC5', 'LFC6', 'LFC7', 'LFC8', 'LFC9', 'LFC10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'LFC1', # (\"Percent Adults in Labor Force\")\n",
    "    'LFC2', # (\"Percent Adult Males in Labor Force\")\n",
    "    'LFC3', # (\"Percent Females in Labor Force\")\n",
    "    'LFC4', # (\"Percent Adult Males Employed\")\n",
    "    'LFC5', # (\"Percent Adult Females Employed\")\n",
    "    'LFC6', # (\"Percent Mothers Employed Married and Single\")\n",
    "    'LFC7', # (\"Percent 2 Parent Earner Families\")\n",
    "    'LFC8', # (\"Percent Single Mother w/ Child in Labor Force\")\n",
    "    'LFC9', # (\"Percent Single Father w/ Child in Labor Force\")\n",
    "    'LFC10' # (\"Percent Families w/ Child w/ no Workers\")\n",
    "]\n",
    "\n",
    "savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**These 0 and 99 values are weird. Delete them. Also remove 50s from LFC9**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans\n",
    "for feature in features: \n",
    "    remove_value(feature, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 99s with nans\n",
    "for feature in features: \n",
    "    remove_value(feature, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 50s with nans for LFC9\n",
    "remove_value('LFC9', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**After removing zeros:**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove value 0 for ACF4 and ACF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with nans for ACF4 and ACF5\n",
    "remove_value('AFC4', 0)\n",
    "remove_value('AFC5', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat the cases where 'LASTDATE' suggests that donor is not lapsing or lapsed (In this dataset, all donors should be either lapsing or lapsed)\n",
    "\n",
    "<span style=\"color:red\">**We have to check this! I feel like I remove too many observations here.**<span>\n",
    "\n",
    "Forum entry \"Lapsed donors\"\n",
    "\n",
    "Entry by David Silva - Thursday, 3 December 2020, 9:43 AM\n",
    " \t\n",
    "\"Hi Philipp,\n",
    "\n",
    "Yes. A lapsed donor is a \"snapshot\" label so it depends on a point in time. The lapsed donors in this dataset are determined according to the date the last promotion (17NK) was emailed to each one of them and the date of their most recent gift. Inconsistent observations are the ones which have an interval between these two dates smaller than 13 months as according to the Lapsed donors description: \"A previous donor who made their donation between 13-24 months ago\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute if flag=True\n",
    "flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Create a series that contains the dates from 'adate_2' substacted by 13 months\n",
    "    adate_2_minus_13_months = donors.ADATE_2.map(lambda x: x - dateutil.relativedelta.relativedelta(months=13))\n",
    "    adate_2_minus_13_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Check if it worked properly\n",
    "    (donors.ADATE_2 - adate_2_minus_13_months) / 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Remove inconsistent values from donors\n",
    "    # Inconsistent values are the ones where 'adate_2_minus_13_months' is before 'LASTDATE'\n",
    "    # All observations that are not inconsistent are kept.\n",
    "    donors_2 = donors[~(adate_2_minus_13_months < donors['LASTDATE'])]\n",
    "\n",
    "    percentage_discarded = ((donors.shape[0] - donors_2.shape[0]) / donors.shape[0])*100\n",
    "    print('Percentage of observation discarded due to inconsistency between their \"lapsed\" status and their LASTDATE value:')\n",
    "    print(round(percentage_discarded, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Accept the changes\n",
    "    donors = donors_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard features that add little additional information\n",
    "**Approach: Identify which variables are highly correlated to each other (pandas-profiling is of great help here) and of these groups plot the covariances and their single and common distributions as well as compute their missing value percentages and their sample standard errors. With this information decide which ones to discard and which ones to keep.**\n",
    "\n",
    "**Workflow: Paste features that are to be looked at into 'features' in the following cell. Then execute all cells of this step. Make decisions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'HC17', # (\"Percent Housing Units w/ Public Water Source\")\n",
    "    'HC18' # (\"Percent Housing Units w/ Well Water Source\")\n",
    "]\n",
    "\n",
    "savefig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    uniques_nans_variance(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans_variance_min_max(features)['Sample standard deviation [Unit of feature]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_visualisation(features, savefig, 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_matrix_visualisation(features, savefig, 'spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_hists(features, savefig, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "many_boxplots(features, savefig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_before = datetime.datetime.now()\n",
    "\n",
    "create_kdeplot = True\n",
    "\n",
    "pairwise(features, create_kdeplot)\n",
    "\n",
    "t_after = datetime.datetime.now()\n",
    "print('Computation time = ', t_after - t_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following direction of pandas profiling (html file in folder \"pandas-profiling/2020-12-15 14/50/43.266760\"),\n",
    "# determine which features to drop right away:\n",
    "to_drop = [\n",
    "    \n",
    "    # 'RFA_2R' # (\"Recency code for RFA_2\") \n",
    "    \n",
    "    # Is a column with a constant value. No variance, no information. Drop it.\n",
    "    \n",
    "    'RFA_2R',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'POP901', # (\"Number of Persons\")\n",
    "    # 'POP902', # (\"Number of Families\")\n",
    "    # 'POP903' # (\"Number of Households\")\n",
    "    \n",
    "    # 'POP902' and 'POP903' are both highly correlated with 'POP901'.\n",
    "    \n",
    "    # None of the three have missing values.\n",
    "    \n",
    "    # Keep 'POP901'. Drop the other two.\n",
    "    \n",
    "    'POP902', \n",
    "    'POP903',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'ETH13', # (\"Percent Mexican\") \n",
    "    # 'ETH5', # (\"Percent Hispanic\")\n",
    "    # 'LSC2' # (\"Percent Spanish Speaking\")\n",
    "    \n",
    "    # None have missing values.\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # ETH13    11.3335\n",
    "    # ETH5     13.7861\n",
    "    # LSC2     12.0437\n",
    "    \n",
    "    # -> Keep 'ETH5'\n",
    "    \n",
    "    'ETH13',\n",
    "    'LSC2',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'AGE901', # (\"Median Age of Population\")\n",
    "    # 'AGE902', # (\"Median Age of Adults 18 or Older\")\n",
    "    # 'AGE903', # (\"Median Age of Adults 25 or Older\"\n",
    "    # 'AGE904', # (\"Average Age of Population\"). Carries slightly less information (in the variance sense) than 'AGE901': Sample standard deviations: 7.2612 years vs. 8.3356 years\n",
    "    # 'AGE905', # (\"Average Age of Adults >= 18\")\n",
    "    # 'AGE906' # (\"Average Age of Adults >= 25\")\n",
    "    \n",
    "    # The latter 5 are highly correlated with 'AGE901'.\n",
    "    \n",
    "    # All have the same amount of missing values.\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # AGE901    7.7426\n",
    "    # AGE902    7.3261\n",
    "    # AGE903    6.9989\n",
    "    # AGE904    6.4949\n",
    "    # AGE905    5.6426\n",
    "    # AGE906    5.3142\n",
    "    \n",
    "    # -> Keep only 'AGE901'\n",
    "    \n",
    "    'AGE902', \n",
    "    'AGE903', \n",
    "    'AGE904', \n",
    "    'AGE905',\n",
    "    'AGE906',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HHAGE1', # (\"Percent Households w/ Person 65+\"\").\n",
    "    # 'AGEC6', # (\"Percent Adults Age 65-74\")\n",
    "    # 'HHAGE2', # (\"Percent Households w/ Person 65+ Living Alone\")\n",
    "    # 'HHAGE3', # (\"Percent Households Headed by an Elderly Person Age 65+\")\n",
    "    \n",
    "    # 'AGEC6', 'HHAGE2', 'HHAGE3' are highly correlated with 'HHAGE1'.\n",
    "    \n",
    "    # These four features have a similar meaning.\n",
    "    \n",
    "    # None have missing values.\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # HHAGE1    13.0903\n",
    "    # AGEC6      6.0038\n",
    "    # HHAGE2     7.4415\n",
    "    # HHAGE3    12.9639\n",
    "    \n",
    "    # -> 'HHAGE1' has the highest variance. Keep it. Drop the rest.\n",
    "    \n",
    "    'AGEC6',\n",
    "    'HHAGE2',\n",
    "    'HHAGE3',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HHN1', # (\"Percent 1 Person Households\")\n",
    "    # 'HHN2', # (\"Percent 2 Person Households\")\n",
    "    # 'HHN3', # (\"Percent 3 or More Person Households\")\n",
    "    # 'HHN4', # (\"Percent 4 or More Person Households\")\n",
    "    # 'HHN5', # (\"Percent 5 or More Person Households\")\n",
    "    # 'HHN6' # (\"Percent 6 Person Households\")\n",
    "    # 3, 4 and 5, 6 are highly correlated with each other and have similar meanings.\n",
    "    # Sample standard deviations:\n",
    "    # HHN3: 14.5385 %\n",
    "    # HHN4: 11.0592 %\n",
    "    # -> Keep HHN3\n",
    "    # HHN5: 6.3828 %\n",
    "    # HHN6: 3.793 %\n",
    "    # -> Keep HHN5\n",
    "    # -> Drop HHN4, HHN6:\n",
    "    \n",
    "    'HHN4',\n",
    "    'HHN6',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'MARR3', # (\"Percent Widowed\")\n",
    "    # 'AGEC7' # (\"Percent Adults Age >= 75\")\n",
    "    \n",
    "    # When there are many old people, there are also many widowed people.\n",
    "    \n",
    "    # Both have no nan values.\n",
    "    \n",
    "    # -> Drop the one with smaller variance.\n",
    "    \n",
    "    # Sample standard deviation:\n",
    "    # MARR3: 4.8877 %\n",
    "    # AGEC7: 6.7237 %\n",
    "    \n",
    "    'MARR3',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HHP1', # (\"Median Person Per Household\")\n",
    "    # 'HHP2', # (\"Average Person Per Household\")\n",
    "    # 'HHN3', # (\"Percent 3 or More Person Households\")\n",
    "    # 'HHN4', # (\"Percent 4 or More Person Households\")\n",
    "    # 'HHD1', # (\"Percent Households w/ Related Children\")\n",
    "    # 'RHP3' # (\"Median Number of Persons per Housing Unit\")\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # HHP1    50.0412 people\n",
    "    # HHP2    49.9018 people\n",
    "    # HHN3    14.5385 %\n",
    "    # HHN4    11.0592 %\n",
    "    # HHD1    13.0351 %\n",
    "    # RHP3     2.5598 people\n",
    "    \n",
    "    # My personal opinion: 'HHD1' has a different meaning to the other 5. Let's keep it.\n",
    "    \n",
    "    # But the other 5 are pretty similar to each other.\n",
    "    \n",
    "    # None of them have missing values.\n",
    "    \n",
    "    # 'RHP3' and 'HHP2' have a lower variance than 'HHP1'. Let's drop them.\n",
    "    # 'HHN4' has a lower variance than 'HHN3'. Let's drop it.\n",
    "    \n",
    "    # Out of 'HHP1' and 'HHN3', 'HHP1' is less correlated with 'HHD1' (which we decided to keep). \n",
    "    # This holds for both Pearson and Spearman correlation. So let's drop 'HHN3'\n",
    "    \n",
    "    'HHP2',\n",
    "    'HHN4',\n",
    "    'RHP3',\n",
    "    'HHN3',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'DW1',   # Percent Single Unit Structure\n",
    "    # 'DW2',   # Percent Detached Single Unit Structure\n",
    "    # 'DW3',   # Percent Duplex Structure\n",
    "    # 'DW4',   # Percent Multi (2+) Unit Structures\n",
    "    # 'DW5',   # Percent 3+ Unit Structures\n",
    "    # 'DW6',   # Percent Housing Units in 5+ Unit Structure\n",
    "    # 'DW7',   # Percent Group Quarters\n",
    "    # 'DW8',   # Percent Institutional Group Quarters\n",
    "    # 'DW9',   # Non-Institutional Group Quarters\n",
    "    # 'HUPA2', # Percent Housing Units w/ >= 10 Units at the Address\n",
    "    # 'HUPA6'  # Percent Renter Occupied, 5+ Units\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # DW1      24.9746 % (Drop this one and...)\n",
    "    # DW2      26.3531 % (...keep this one)\n",
    "    # DW3       5.3653 % (Pretty independant from the rest. Keep it.)\n",
    "    # DW4      23.8544 % (This one is highly correlated with DW4, DW5, DW6 and HUPA2, HUPA6. Keep DW4 and drop them.)\n",
    "    # DW5      22.6455 % (Drop)\n",
    "    # DW6      20.4578 % (Drop)\n",
    "    # DW7       5.9069 % (Highly correlated with DW8. Keep this one and drop DW8)\n",
    "    # DW8       4.2548 % (Drop)\n",
    "    # DW9       3.9695 % (I assume it's % as well) (Pretty independant from the rest. Keep it.)\n",
    "    # HUPA2    17.1094 % (Drop)\n",
    "    # HUPA6    17.8080 % (Drop)\n",
    "    \n",
    "    'DW1',\n",
    "    'DW5',\n",
    "    'DW6',\n",
    "    'DW8',\n",
    "    'HUPA2',\n",
    "    'HUPA6'\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'RP1', # (\"Percent Renters Paying >= $500 per Month\")\n",
    "    # 'RP2', # (\"Percent Renters Paying >= $400 per Month\")\n",
    "    # 'RP3', # (\"Percent Renters Paying >= $300 per Month\")\n",
    "    # 'RP4', # (\"Percent Renters Paying >= $200 per Month\")\n",
    "    # 'HV4' # (\"Average Contract Rent in hundreds\")\n",
    "    \n",
    "    # These are all highly correlated.\n",
    "    \n",
    "    # Keeping only RP3 since it is amongs the ones with the highest variance (Note that HV4 can not be included in the\n",
    "    # comparison because it is on a totally different scale) and since after the removal of zeros in the previous step\n",
    "    # its distribution seems to be the most well behaved. Dropping the other ones.\n",
    "    \n",
    "    'RP1',\n",
    "    'RP2',\n",
    "    'RP4',\n",
    "    'HV4',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'IC1', # (\"Median Household Income in hundreds\")\n",
    "    # 'IC2', # (\"Median Family Income in hundreds\")\n",
    "    # 'IC3', # (\"Average Household Income in hundreds\")\n",
    "    # 'IC4', # (\"Average Family Income in hundreds\")\n",
    "    # 'IC5' # (\"Per Capita Income\")\n",
    "    \n",
    "    # IC2 has the highest variance out of the first four (Note that IC5 can not be included in the comparison because \n",
    "    # it is on a totally different scale) and it looks like the most well behaved distribution. Keep IC2 and drop the\n",
    "    # other ones.\n",
    "    \n",
    "    'IC1',\n",
    "    'IC3',\n",
    "    'IC4',\n",
    "    'IC5',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'IC6', # (\"Percent Households w/ Income < $15,000\")\n",
    "    # 'IC7', # (\"Percent Households w/ Income $15,000 - $24,999\")\n",
    "    # 'IC8', # (\"Percent Households w/ Income $25,000 - $34,999\")\n",
    "    # 'IC9', # (\"Percent Households w/ Income $35,000 - $49,999\")\n",
    "    # 'IC10', # (\"Percent Households w/ Income $50,000 - $74,999\")\n",
    "    # 'IC11', # (\"Percent Households w/ Income $75,000 - $99,999\")\n",
    "    # 'IC12', # (\"Percent Households w/ Income $100,000 - $124,999\")\n",
    "    # 'IC13', # (\"Percent Households w/ Income $125,000 - $149,999\")\n",
    "    # 'IC14', # (\"Percent Households w/ Income >= $150,000\")\n",
    "    # 'IC15', # (\"Percent Families w/ Income < $15,000\")\n",
    "    # 'IC16', # (\"Percent Families w/ Income $15,000 - $24,999\")\n",
    "    # 'IC17', # (\"Percent Families w/ Income $25,000 - 34,999\")\n",
    "    # 'IC18', # (\"Percent Families w/ Income $35,000 - $49,999\")\n",
    "    # 'IC19', # (\"Percent Families w/ Income $50,000 - $74,999\")\n",
    "    # 'IC20', # (\"Percent Families w/ Income $75,000 - $99,999\")\n",
    "    # 'IC21', # (\"Percent Families w/ Income $100,000 - $124,999\")\n",
    "    # 'IC22', # (\"Percent Families w/ Income $125,000 - $149,999\")\n",
    "    # 'IC23' # (\"Percent Families w/ Income >= $150,000\")\n",
    "    \n",
    "    # Naturally, IC6 is highly correlated with IC15 and so on. So having both sets of features would be quite \n",
    "    # redundant. \n",
    "    \n",
    "    # Looking at the variance (see sample standard deviations table below), it could be said that IC6 / IC 15 and\n",
    "    # IC10 / IC 19 give the most information. The first is an indicator for the amount of poor households, the latter\n",
    "    # an indicator for the amount of \"medium\" households. Idea: Keep one of each: The one with the higher variance, so\n",
    "    # keep IC6 and IC19 and drop the rest. IC6 and IC19 have rather well behaved distributions as well.\n",
    "    \n",
    "    # Sample standard deviations: \n",
    "    # IC   \"Household\"   IC  \"Families\"\n",
    "    # 6    14.2979     15    12.1377\n",
    "    # 7    7.6694      16    8.8146\n",
    "    # 8    5.9860      17    7.2545\n",
    "    # 9    7.1408      18    8.0242\n",
    "    # 10   9.4480      19    10.2369\n",
    "    # 11   5.8353      20    6.6662\n",
    "    # 12   3.3463      21    3.9072\n",
    "    # 13   1.9231      22    2.3217\n",
    "    # 14   4.6407      23    5.5313\n",
    "    \n",
    "    'IC7',\n",
    "    'IC8',\n",
    "    'IC9',\n",
    "    'IC10',\n",
    "    'IC11',\n",
    "    'IC12',\n",
    "    'IC13',\n",
    "    'IC14',\n",
    "    'IC15',\n",
    "    'IC16',\n",
    "    'IC17',\n",
    "    'IC18',\n",
    "    'IC20',\n",
    "    'IC21',\n",
    "    'IC22',\n",
    "    'IC23',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HHAGE1' # (\"Percent Households w/ Person 65+\") (Decided to keep this one, see above in this cell.)\n",
    "    # 'HHAS1' # (\"Percent Households on Social Security\")\n",
    "    # 'HHAS2' # (\"Percent Households on Public Assistance\")\n",
    "    # 'HHAS3' # (\"Percent Households w/ Interest, Rental or Dividend Income\")\n",
    "    # 'HHAS4' # (\"Percent Persons Below Poverty Level\")\n",
    "    \n",
    "    # HHAGE1 and HHAS1 are highly correlated. HHAS1 looks more well behaved. Keep HHAS1 Drop HHAGE1.\n",
    "    \n",
    "    'HHAGE1' # (\"Percent Households w/ Person 65+\")\"\n",
    "    \n",
    "    # Confirming this decision by double checking with this combination of features:\n",
    "    \n",
    "    # 'HHAS1' # (\"Percent Households on Social Security\") \n",
    "    # 'AGEC6', # (\"Percent Adults Age 65-74\")\n",
    "    # 'HHAGE1', # (\"Percent Households w/ Person 65+\"\").\n",
    "    # 'HHAGE2', # (\"Percent Households w/ Person 65+ Living Alone\")\n",
    "    # 'HHAGE3', # (\"Percent Households Headed by an Elderly Person Age 65+\")\n",
    "    \n",
    "    # These are all highly correlated.\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # HHAS1     13.6223\n",
    "    # AGEC6      5.9319\n",
    "    # HHAGE1    12.9246\n",
    "    # HHAGE2     7.4415\n",
    "    # HHAGE3    12.8220\n",
    "    \n",
    "    # A look at the combination above confirms the decision to keep HHAS1 since it has the highest variance out of\n",
    "    # these ones.\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'TPE1', # (\"Percent Driving to Work Alone Car/Truck/Van\")\n",
    "    # 'TPE2', # (\"Percent Carpooling Car/Truck/Van)\")\n",
    "    # 'TPE3', # (\"Percent Using Public Transportation\")\n",
    "    # 'TPE4', # (\"Percent Using Bus/Trolley\")\n",
    "    # 'TPE5', # (\"Percent Using Railways\")\n",
    "    # 'TPE6', # (\"Percent Using Taxi/Ferry\")\n",
    "    # 'TPE7', # (\"Percent Using Motorcycles\")\n",
    "    # 'TPE8', # (\"Percent Using Other Transportation\")\n",
    "    # 'TPE9' # (\"Percent Working at Home/No Transportation\")\n",
    "    \n",
    "    # Could think about dropping either TPE3 or TPE4 since they are correlated.\n",
    "    \n",
    "    # Also note: If used, it would be good to delete the zeros at least from TPE1 and TPE2\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'LFC1', # (\"Percent Adults in Labor Force\")\n",
    "    # 'LFC2', # (\"Percent Adult Males in Labor Force\")\n",
    "    # 'LFC3', # (\"Percent Females in Labor Force\")\n",
    "    # 'LFC4', # (\"Percent Adult Males Employed\")\n",
    "    # 'LFC5', # (\"Percent Adult Females Employed\")\n",
    "    # 'LFC6', # (\"Percent Mothers Employed Married and Single\")\n",
    "    # 'LFC7', # (\"Percent 2 Parent Earner Families\")\n",
    "    # 'LFC8', # (\"Percent Single Mother w/ Child in Labor Force\")\n",
    "    # 'LFC9', # (\"Percent Single Father w/ Child in Labor Force\")\n",
    "    # 'LFC10' # (\"Percent Families w/ Child w/ no Workers\")\n",
    "    \n",
    "    # LFC1,2,3,4,5 are highly correlated. Keep LFC4 (highest variance). Drop the rest.\n",
    "    \n",
    "    'LFC1',\n",
    "    'LFC2',\n",
    "    'LFC3',\n",
    "    'LFC4',\n",
    "    'LFC5',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "\n",
    "    # 'EIC1', # (\"Percent Employed in Agriculture\")\n",
    "    # 'OCC9' # (\"Percent Farmers\")\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # EIC1    5.5708\n",
    "    # OCC9    5.1068\n",
    "    \n",
    "    # -> Keep EIC1, drop OCC9\n",
    "    \n",
    "    'OCC9',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'AFC1', # (\"Percent Adults in Active Military Service\")\n",
    "    # 'AFC2', # (\"Percent Males in Active Military Service\")\n",
    "    # 'AFC3', # (\"Percent Females in Active Military Service\")\n",
    "    # 'AFC4', # (\"Percent Adult Veterans Age 16+\")\n",
    "    # 'AFC5', # (\"Percent Male Veterans Age 16+\")\n",
    "    # 'AFC6' # (\"Percent Female Veterans Age 16+\")\n",
    "    \n",
    "    # Sample standard deviations:\n",
    "    # AFC1     3.1694\n",
    "    # AFC2     4.9033\n",
    "    # AFC3     1.0661\n",
    "    # AFC4     5.2705\n",
    "    # AFC5    10.4757\n",
    "    # AFC6     1.6404\n",
    "    \n",
    "    # ACF4 and ACF5 are highly correlated. Drop AFC4 (lower variance).\n",
    "    # Same for ACF1 and ACF2. Drop ACF1.\n",
    "    \n",
    "    'AFC1', # (\"Percent Adults in Active Military Service\")\n",
    "    'AFC4', # (\"Percent Adult Veterans Age 16+\")\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HC1',\n",
    "    # 'HC2',\n",
    "    # 'HC3',\n",
    "    # 'HC4',\n",
    "    # 'HC5',\n",
    "    # 'HC6',\n",
    "    # 'HC7', # (\"Percent Owner Occupied Structures Built Since 1980\")\n",
    "    # 'HC8' # (\"Percent Owner Occupied Structures Built Prior to 1860\")\n",
    "    \n",
    "    # HC7, HC8 are highly negatively correlated.\n",
    "    \n",
    "    # Consider dropping HC3, ..., HC8. Not sure about HC1 and HC2.\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'HC17', # (\"Percent Housing Units w/ Public Water Source\")\n",
    "    # 'HC18' # (\"Percent Housing Units w/ Well Water Source\")\n",
    "    \n",
    "    # Highly negatively correlated. Dopping one.\n",
    "    \n",
    "    'HC18',\n",
    "\n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'CARDPROM', # (\"Lifetime number of card promotions received to date.\")\n",
    "    # 'NUMPROM', # (\"Lifetime number of promotions received to date\")\n",
    "    \n",
    "    # Highly correlated. Drop one.\n",
    "    \n",
    "    'CARDPROM',\n",
    "    \n",
    "    #====================\n",
    "    #====================\n",
    "    \n",
    "    # 'CARDGIFT', # (\"Number of lifetime gifts to card promotions to date\")\n",
    "    # 'NGIFTALL', # (\"Number of lifetime gifts to date\")\n",
    "    \n",
    "    # Highly correlated. Drop one.\n",
    "    \n",
    "    'CARDGIFT'\n",
    "    \n",
    "]\n",
    "\n",
    "print('Number of features to drop:', len(to_drop))\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values for all metric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For now, let's just fill all missing values with the features median**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in metric_features:\n",
    "    without_nans = donors[feature].dropna()\n",
    "    if feature in date_features:\n",
    "        median = sorted(without_nans)[len(without_nans)//2]\n",
    "    else:\n",
    "        median = without_nans.median()\n",
    "    donors[feature].fillna(value=median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values for the metric features\n",
    "donors[metric_features].isna().sum()[donors[metric_features].isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**No more nan values present in the metric features**<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of nan values once more\n",
    "donors.isna().sum()[donors.isna().sum()!=0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert date features to numbers\n",
    "**The idea here is to use the value of 'ADATE_2' (Date the 17NK promotion was mailed) of every obersvation as reference date.**\n",
    "\n",
    "**The new values will be the number of days before the reference date. So for example DOB will be turned into the age in days relative to reference date.**\n",
    "\n",
    "**Additionally - for better understandability - all transformed columns are renamed to ......_rel_in_days**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies\n",
    "adate_2 = donors['ADATE_2'].copy()\n",
    "date_features_copy = date_features.copy()\n",
    "metric_features_copy = metric_features.copy()\n",
    "\n",
    "for feature in date_features:\n",
    "    \n",
    "    rel_time = adate_2 - donors[feature]\n",
    "    rel_time_days = rel_time.dt.days\n",
    "    donors[feature] = rel_time_days\n",
    "    \n",
    "    # Attach '_rel_in_days' to column name\n",
    "    new_col_name = feature + '_rel_in_days'\n",
    "    donors.rename(columns={feature : new_col_name}, inplace=True)\n",
    "    \n",
    "    # Update our lists 'metric_features' and 'date_features', Step 1\n",
    "    date_features_copy[date_features_copy.index(feature)] = new_col_name\n",
    "    metric_features_copy[metric_features_copy.index(feature)] = new_col_name\n",
    "    \n",
    "# Update our lists 'metric_features' and 'date_features', Step 2\n",
    "date_features = date_features_copy\n",
    "metric_features = metric_features_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors['ADATE_10_rel_in_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors['ADATE_2_rel_in_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "<span style=\"color:red\">**This part needs work</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some descriptive statistics\n",
    "desc_stats = donors.describe(include=\"all\")  # try with all and without all\n",
    "# desc_stats = donors.describe()  # try with all and without all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a certain one\n",
    "feature = 'DOB_rel_in_days'\n",
    "desc_stats[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[feature][~donors[feature].isna()].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric features that are in string format: Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data types of metric features, excluding the features defined in 'date_features'\n",
    "dtypes_metric_without_dates = donors[list(set(metric_features) - set(date_features))].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features from 'dtypes_metric_without_dates' that have type 'object'\n",
    "dtypes_metric_without_dates[dtypes_metric_without_dates=='O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Due to the removal of features with high missing-values percentage, no metric features in string format are left**</span>\n",
    "\n",
    "<span style=\"color:red\">**Maybe the function below will be handy for something else**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert numerical String features to float\n",
    "def to_num(df, colname):\n",
    "    \"\"\"This function takes a dataframe and a column name and converts the column with this name to float\"\"\"    \n",
    "    df[colname] = pd.to_numeric(df[colname],errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_multiplier = 1.5\n",
    "\n",
    "# Get the quartiles. \"numeric_only=False\" so we also get quartiles for datetime columns.\n",
    "q25 = donors[metric_features].quantile(0.25, axis=0, numeric_only=False)\n",
    "q75 = donors[metric_features].quantile(0.75, axis=0, numeric_only=False)\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "upper_lim = q75 + iqr_multiplier * iqr\n",
    "lower_lim = q25 - iqr_multiplier * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features: One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[non_metric_features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(donors[non_metric_features]['MDMAUD_R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

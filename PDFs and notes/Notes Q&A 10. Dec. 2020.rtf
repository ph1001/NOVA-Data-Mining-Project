{\rtf1\ansi\ansicpg1252\cocoartf2577
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13600\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs32 \cf0 Notes Q&A 10. Dec. 2020
\fs24 \
\

\f1\b My question:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 There are many categorical variables in the dataset. What would you suggest we do with them?\
\
- Answer:\
\
	- We only use the metric features for clustering.\
		- Metric features are intervalar and ordinal variables.\
	- After we have the clustering, we can then use the categorical features for interpreting our clusters.\
	- For interpretation purposes, we should one-hot encode them.\
\
- Using categorical features for clustering could be part of the self exploration part. There are some techniques such as:\
K prototypes approach\
\
The clustering can be done bottom up or top down.\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Look up:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 \'93Local outlier factor\'94\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Link posted by Joao:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 More robust detection of outliers:
\f0\b0 \
- Use multiple outlier detection techniques which all yield a table that define which points are outliers and which ones aren\'92t.\
- Use the intersection or average or something like that to get a robust estimate on which points are outliers.\
\
=======\
\

\f1\b Pandas profiling:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 It is an open source software, so be critical of its result.\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 R-squared:
\f0\b0 \
- The R-squared metric is only valid if we compare clusterings on the same dataset and for the same number of clusters.\
- It\'92s only good for comparing different clustering solutions, not to see if a clustering is a good clustering by itself.\
\
- Another metric: The difference in means\
\
=======\
\

\f1\b IQR Method:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 The choice of the multiplier is arbitrary. Can be any number.\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Possible approach:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 - First: use hierachachical clustering to assess the number of clusters\
- Then: Use for example k-means\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 The report:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 \
- Content (not sure if I got everything):\
	- Exploratory analys\
	- Why did we do the preprocessing that we did\
	- How did we do our clustering\
	- What are our final solutions\
	- Which marketing strategy for which cluster?\
\
- The report can be done in Word. Or in Latex or anything we want. It\'92s our choice.\
\
- Don\'92t explain all the features that we dropped one-by-one. Just say which variables were dropped with method 1 (and why), which ones were dropped with method 2 (and why), etc.\
\
- Explain which variables were kept and why.\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Joao: \'93A very relevant question that was asked by some students\'94:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 K-neared-neighbour imputer. To use all the features or just some?\
\
Answer:\
Try to avoid using too many features. Many features means sparse space. \
Solution: Take a set of features that are a well representing for this purpose.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 KNN Imputer:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 - We can subjectively choose the features that we use for KNN Imputing\
- A good choice are features that are highly correlated with the feature that we want to fill.\
\
=======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Reference date:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 Use the date of promotion number 2 of each customer as the reference date.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 How many metric features do we use for our final clustering?\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 - Not less than five. But other than that, it\'92s our choice, as long as we explain it well. Probably more than 15 doesn\'92t make much sense, so 15 is probably the maximum (Joao said).\
- In the interpretation part, we can use as many categorical 
\f1\b and also metric features (!)
\f0\b0  as we want.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Columns that have over 40% missing values:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 - Unless they are columns that are extremely important, drop them.\
- With this many missing values (over 40%), a very solid imputing method would be needed. Something more powerful than just KNN. Like a proper supervised learning approach (Whatever this means).\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Knowledge about the algorithms:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 We might be asked how the algorithms work that we used (conceptually). Especially if we use algorithms that weren\'92t covered in the classes.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Generally: \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 Make sure to standardise the data\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Joao:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 \'93If a feature is not correlated with any of the other features, it is probably not that important.\'94
\f1\b \
\

\f0\b0 ======\
\
Correlation higher than 90%:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Maybe
\f0\b0  drop one. Use critical thinking! Maybe they don\'92t say the same thing and should both be kept!\
\

\f1\b \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 ======\
\
What to do when an observation hasn\'92t sent a gift yet?\
We could impute it as zero.\
But be careful: By imputing zero, we make it similar to someone with a value close to zero. But in fact it isn\'92t such a similar situation.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 If we use an algorithm that is not so well known:
\f0\b0 \
Write some information about how it works and put it in the appendix.\
\
======\
\

\f1\b is it okay if there are some features that we end up not using at all in the whole project?\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 Yes.\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 Domain knowledge:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 Research and knowledge on the domain (that has implications on what we did) will also be valued in the report.\
\
\
======\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 PCA:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 Run PCA between \'91related\'92 features. (ex: population, wealthiness, region) - this way we reduce dimensionality but keep information\
}